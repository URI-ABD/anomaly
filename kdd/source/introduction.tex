\section{Introduction}
\label{sec:introduction}

Detecting anomalies or outliers from a distribution of data is a well-studied problem in machine learning.
When data occupy easily-described distributions, such as Gaussian, the task is relatively easy, requiring only that one identify that a datum is sufficiently far from the median or mean. 
However, in ``big data'' scenarios, where data can occupy high-dimensional spaces, anomalous behavior becomes harder to quantify.
If the data happens to be uniformly distributed throughout even a high dimensional space one can conceive of simple mechanisms that would be effective, such as a Single-Class SVM, yet data rarely exhibits such patterns.
Instead, data often obey the ``manifold hypothesis''~\cite{fefferman2016testing}, occupying a low-dimensional manifold in the higher-dimensional embedding space.
This low-dimensional manifold may weave itself through the higher dimensional space much like a crumpled piece of paper can appear to be a sphere from a distance.
Detecting anomalies or outliers in such a landscape is not so easy; in particular, correctly identifying an anomalous datum that sits within the gaps of a manifold presents a challenge.

Anomalies (data which don't belong to a particular distribution) and outliers (data which represent extrema of a distribution) can arise from many sources: errors in measurement or data collection; novel, previously-unseen instances of data; normal behavior evolving into abnormal; or adversarial attacks as seen in examples of malicious inputs to machine-learning systems~\cite{schoolbus-ostrich}.
Current systems designed to detect anomalous behavior systematically fail for a wide variety of reasons.
While some systems perform well with large quantities of data, such as grid-based approaches, they falter as dimensionality increases.
Others may succeed with a larger number of dimensions, but perform too slowly to be truly useful.

\subsection{Clustering-Based Approaches}

Clustering data is universally centered around techniques for grouping instances in a way that provides value.
This is generally done by assigning \textit{similar} points to the same cluster.
Once clusters are formed, they can be used to inspect the data in great detail.
For example, for any given point, one could estimate it's anomalousness by it's distance to it's nearest cluster.
To determine if a cluster contains outliers, one could examine the relative cardinality of the cluster.

We summarize several major approaches to clustering data.
Note that there have been relatively few advancements in clustering techniques over the past decade~\cite{wang_progress_2019}. This may explain, or by attributable to, the observably poor performance, thus far, of clustering data in higher dimensional space~\cite{zhang_advancements_2013}.

\subsubsection{Distance-Based Clustering}

Distance-based clustering approaches rely on some distance measure to partition the data into some number of clusters.
Within this approach, the numbers of clusters and/or the sizes of clusters are normally predefined: either user-specified, or chosen at random~\cite{wang_progress_2019}.
Some examples of distance-based clustering are: 
K-Means~\cite{macqueen_methods_nodate}, 
PAM~\cite{kaufman_finding_nodate}, 
CLARANS~\cite{ng_efficient_nodate}, 
CLARA~\cite{kaufman_finding_nodate}, 
etc.

\subsubsection{Hierarchical Clustering}

Hierarchical clustering methods typically utilize a tree-like structure, where points are allocated into leaf nodes~\cite{wang_progress_2019}.
These tree-like structures can be created bottom-up (agglomerative clustering), or top-down (divisive clustering)~\cite{rakesh_agrawal_automatic_1998}.
The major drawback of these methods is the high cost of pairwise difference computations at each level of the tree.
Examples of hierarchical clustering include: 
MST~\cite{charles_zahn_graph_1971}, 
CURE~\cite{noauthor_cure:_nodate}, 
CHAMELEON~\cite{karypis_chameleon:_nodate}, 
etc.

\subsubsection{Density-Based Clustering}

Density-Based clustering methods rely on finding areas of high point-density separated by areas of low point-density.
These algorithms generally do not work well when normal data are sparse or uniformly distributed.
Some examples of density-based clustering algorithms are: 
DBSCAN~\cite{ester_density-based_nodate}, 
DENCLUE~\cite{Hinneburg1998Effic-5816}, 
etc.

\subsubsection{Grid-Based Clustering}

Grid-based clustering works via segmenting the entire space into a finite number of cells and then iterating over the cells to find regions of higher density.
Utilizing the grid structure for clustering means that these algorithms have typically scaled better to larger datasets.
Some examples of grid-based clustering: 
STING~\cite{sting:wang}, 
Wavecluster~\cite{Wavecluster:Sheikholeslami:2000}, 
CLIQUE~\cite{rakesh_agrawal_automatic_1998}, 
etc.

\subsection{Distanced-Based Approaches}

Distance-based methods attempt to find anomalous points via distance comparisons.
These methods largely employ k-Nearest Neighbors as their substrate~\cite{wang_progress_2019}.
They proceed in the following, slightly different, ways:

\begin{enumerate}
    \item Points with less than $p$ other points within a distance $d$ are outliers.
    \item The top $n$ points whose distances to their $k^{th}$-nearest neighbor are the greatest are outliers.
    \item The top $n$ points whose average distances to their $k^{th}$-nearest neighbor are the greatest are outliers.
\end{enumerate}

\subsection{CHAODA}

In this paper we introduce a novel technique, Clustered Learning of Approximate Manifolds (CLAM).
This approach uses a divisive hierarchical clustering approach to learn a manifold in a Banach space~\cite{} defined by a distance metric.
In actuality, our requirements are less strict; the space can be defined by a distance \textit{function} that does not obey the triangle inequality, though this is not always optimal. 
Given a learned approximate manifold, we can \textit{almost} trivially implement several anomaly-detection algorithms; in this manuscript, we present a collection of five such algorithms implemented on CLAM: CHAODA (Clustered Hierarchical Anomaly and Outlier Detection Algorithms).

The manifold learning component derives from prior work aimed at accelerating the task of approximate search on large data sets of high dimension, CHESS~\cite{ishaq2019entropy}.
CLAM begins by divisively clustering the data until every point is within its own cluster, as a singleton.
CLAM then delineates \textit{layers} of clusters at each depth in the tree.
Each layer comprises all clusters that would have been leaf nodes if the tree building were to have been halted at the given depth.

CLAM then build a graph for each layer in the tree by creating edges between clusters that have overlapping volumes.
This process effectively learns the manifold on which the data lie at various resolutions, given by the depth of the layer.
This is analogous to a ``filtration'' in computational topology~\cite{carlsson2009topology}.
Once we have learned a manifold, one can ask about the cardinality of various clusters at different depths, how connected a given cluster is, or even how often a cluster is visited over many different random walks across the manifold.

We test our methods on 26 real-world datasets. 
Each dataset contains a different number of anomalous data, for a different domain.
We consider several different definitions for outliers and anomalies: \textbf{distance-based}, examining several classical distance-based definitions of outliers, relying on CLAM's use of distance to cluster data; \textbf{density-based}, examining the cardinality of clusters, under the hypothesis that clusters with lower cardinality are more likely to contain outliers; \textbf{graph-based}, examining several graph-theoretic methods for anomaly detection, given graphs constructed from layers of clusters.

Historically, clustering approaches have suffered from one several problems.
The most common deficiencies are: the effective treatment of high dimensionality, the ability to interpret results, and the ability to scale to exponentially-growing datasets ~\cite{rakesh_agrawal_automatic_1998}.
With CLAM, we have largely alleviated these problems.
