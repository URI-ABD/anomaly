\section{Methodology}
\label{sec:methodology}

Given the manifold and induced graph learned by the CLAM approach described in Section~\ref{introduction:chaoda}, we have implemented five distinct algorithms for detecting anomalies or outliers, all of which take advantage of either the tree of hierarchical clusters or the graph learned from those clusters at a given depth.


\subsection{Cluster Cardinality}

For this method we equate anomalousness with the cardinality of the cluster to which a point belongs.
If a point belongs to a cluster with a very small number of points, it is more likely that the point is an anomaly or an outlier.
Clusters that have a large cardinality are treated as more likely to be normal. Algorithm~\ref{alg-cc} details the cluster cardinality approach.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwData{manifold, depth}
\KwResult{scores}
 $scores \leftarrow \{\}$\;
 $d \leftarrow depth$\;
 \ForAll{cluster $c \in$ manifold.graph(d).clusters}{
  \ForAll{$p \in c.points$}{
   $scores[p] \leftarrow |c|$
  }
 }
 normalize(scores)\;
 \ForAll{$p \in scores$}{$scores[p] \leftarrow 1-p$}
 \caption{Cluster Cardinality}
 \label{alg-cc}
\end{algorithm}

\subsection{Hierarchical}

In this approach, we examine the ratio of the cardinality of a cluster and that of its parent cluster.
For a given depth of the tree, a point is considered to be more anomalous if the cluster it belongs to at that depth is significantly smaller than the cluster it belongs to one level higher in the tree.
This relationship tries to capture the intution that the point lives away from the manifold inhabited by most of the data. This approach is detailed in Algorithm~\ref{alg-hierarchical}.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwData{manifold,depth}
\KwResult{scores}
 $scores \leftarrow \{\}$\;
 $d \leftarrow depth$\;
 \For{$d \in 1\cdots$ depth}{
  \ForAll{cluster $c \in $manifold.graph(d).clusters}{
    \ForAll{point $p \in c$}{
     $scores[p] \leftarrow \frac{|c.parent|}{|c|\times d}$
    }
  }
 }
 normalize(scores)\;
 \caption{Hierarchical}
 \label{alg-hierarchical}
\end{algorithm}

\subsection{k-Neighborhood}

For this approach, we rely on the graph learned by CLAM at a particular depth of the tree, as explained in Section~\ref{introduction:chaoda}. Given that graph, and a point in question, we consider the number of clusters reachable within a graph distance of $k$ from the point's cluster.
The more clusters which are reachable from a given cluster, the \textit{less} likely it is to contain anomalous points. This approach is detailed in Algorithm~\ref{alg-kneighborhood}

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwData{manifold,depth,k}
\KwResult{scores}
 $scores \leftarrow \{\}$\;
 $d \leftarrow $ depth\;
 $g \leftarrow $ manifold.graph(d).clusters\;
 \ForAll{cluster $c \in g$}{
  $v \leftarrow |\{c' \in g | \delta(c,c') \le k\}|$\;
  \ForAll{$p \in c$}{
   scores[p] $\leftarrow v$\;
  }
 }
 normalize(scores)\;
 \ForAll{$p \in scores$}{$scores[p] \leftarrow 1-p$}
 \caption{k-Neighborhood}
 \label{alg-kneighborhood}
\end{algorithm}

\subsection{Random Walk}

Here, we implement the Outrank~\cite{moonesinghe2008outrank} algorithm, with a key difference: since the graph learned by CLAM at a given depth of the tree may contain more than one connected component, some parts of the graph may be completely unreachable from a point in question.
The general idea behind this approach is to examine the frequency with which clusters are visited during $n$ random walks.
Clusters that are visited more frequently, are again less likely to be anomalous.
Clusters that are not visited frequently are more likely to be disconnected, and distant from the underlying manifold. This approach is detailed in Algorithm~\ref{alg-rw}, which assumes that $outrank(k)$ returns a hash table whose keys are the clusters of the graph, and whose values are the number of visits by a random walk to each cluster.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwData{manifold,depth}
\KwResult{scores}
 $scores \leftarrow \{\}$\;
 $d \leftarrow $ depth\;
 $g \leftarrow $ manifold.graph(d).components\;
 \ForAll{component $k \in g$}{
  visits $\leftarrow$ outrank(k)\;
  \ForAll{cluster $c \in k$}{
   $v \leftarrow $visits[c]\;
   \ForAll{$p \in c$}{
    scores[p] $\leftarrow v$\;
   }
  }
 }
 normalize(scores)\;
 \ForAll{$p \in scores$}{$scores[p] \leftarrow 1-p$}
 \caption{Random Walk}
 \label{alg-rw}
\end{algorithm}

\subsection{Subgraph Cardinality}

This approach is similar to Cluster Cardinality. Here,  we utilize the cardinality of each connected component of the graph at various depths to determine anomalousness.
Similarly to some of our other approaches, we postulate that clusters which are members of sparse components are themselves more likely to be anomalous. Algorithm~\ref{alg-sgc} details this approach.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\KwData{manifold, depth}
\KwResult{scores}
 $scores \leftarrow \{\}$\;
 $d \leftarrow depth$\;
 \ForAll{component $k \in$ manifold.graph(d).components}{
  \ForAll{cluster $c \in k$}{
   \ForAll{$p \in c$}{
    scores[p] $\leftarrow |k|$
   }
  }
 }
 normalize(scores)\;
 \ForAll{$p \in scores$}{$scores[p] \leftarrow 1-p$}
 \caption{Subgraph Cardinality}
 \label{alg-sgc}
\end{algorithm}

\subsection{Normalization Methods}

Due to the wide range of possible measurements for ``anomalousness'' from our methods, we normalize our measurements.

\begin{enumerate}
    \item Min-Max Scaling:
    \begin{gather}
        x^{\prime} = \frac{x - x_{min}}{x_{max} - x_{min}}
        \label{sec:methods:min-max-normalizationn}
    \end{gather}
    
    \item Mean-Scaling:
    \begin{gather}
        x^{\prime} = \frac{x - \overline{x}}{x_{max} - x_{min}}
        \label{sec:methods:mean-scaling}
    \end{gather}
    
    \item z-Score Standardization:
    \begin{gather}
        x^{\prime} = \frac{x - \overline{x}}{\sigma}
        \label{sec:methods:z-score}
    \end{gather}

\end{enumerate}
